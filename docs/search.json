{
  "articles": [
    {
      "path": "about.html",
      "title": "About",
      "author": [],
      "contents": "\r\nHi I am Siwei\r\n\r\n\r\n\r\n",
      "last_modified": "2021-11-17T11:01:17-06:00"
    },
    {
      "path": "blog.html",
      "title": "Blog",
      "author": [],
      "contents": "\r\n\r\n\r\n\r\n",
      "last_modified": "2021-11-17T11:01:20-06:00"
    },
    {
      "path": "final.html",
      "title": "Final Project",
      "author": [],
      "date": "`r Sys.Date()`",
      "contents": "\r\nI. About the Dataset (Describtion, source, and what purposes)\r\n\r\nDescribtion:\r\nSource:\r\nPurpose:\r\n\r\nII. Descriptive Stats: small discussion (NA, outliers, skewed)\r\n\r\n\r\n\r\nMissing Values\r\n\r\n\r\n# missing values\r\n# summary(BankChurners)\r\nBankChurners1 <- unknownToNA(BankChurners, unknown = c(\"Unknown\", \"\"))\r\n# summary(BankChurners1)\r\ntable(is.na(BankChurners1))\r\n\r\n\r\n\r\n FALSE   TRUE \r\n203003   9664 \r\n\r\n# sapply(BankChurners1, function(x) sum(is.na(x)))\r\nBankChurners2 <- knnImputation(BankChurners1)\r\n# summary(BankChurners2)\r\ntable(is.na(BankChurners2))\r\n\r\n\r\n\r\n FALSE \r\n212667 \r\n\r\nResampling\r\n\r\n\r\n# imbalanced dataset\r\ntable(BankChurners2$Attrition_Flag)\r\n\r\n\r\n\r\nExistingCustomer AttritedCustomer \r\n            8500             1627 \r\n\r\nBankChurners2_rose <- ROSE(Attrition_Flag ~., data = BankChurners2)$data\r\ntable(BankChurners2_rose$Attrition_Flag)\r\n\r\n\r\n\r\nExistingCustomer AttritedCustomer \r\n            5011             5116 \r\n\r\n# summary(BankChurners2_rose)\r\n# str(BankChurners2_rose)\r\n# View(BankChurners2_rose)\r\n# BankChurners2_rose %>% \r\n#   mutate_if(is.factor, ~ factor(trimws(., whitespace = \"\\\\s*-.*\")))\r\n\r\n\r\n\r\nIII. Describe the model\r\n\r\nGradient Boosting Tree:\r\nRandom Forest:\r\n\r\nIV. Describe the process and results\r\n\r\nProcess:\r\nResults:\r\n\r\nV. Code (summary, split data, run, results)\r\nshould I use PCA before RF bcz it takes forever to run RF\r\n\r\n\r\n# caret-PCA\r\n# BankChurners_pca <- BankChurners2_rose\r\n# # BankChurners_pca <- as.numeric(BankChurners2_rose)\r\n# BankChurners2_pca <- preProcess(BankChurners_pca, method = \"pca\", pcaComp = 12)\r\n# BankChurners2_pca\r\n# BankChurners2_pca_stat <- prcomp(BankChurners_pca[, -2])\r\n# plot(BankChurners2_pca_stat, type = \"l\")\r\n\r\n# FactoMineR-PCA\r\n# BankChurners2_rose_pca <- BankChurners2_rose\r\n# BankChurners_pca <- PCA(BankChurners2_rose_pca, \r\n#                         ncp = 5,\r\n#                         ind.sup = 2,\r\n#                         quali.sup = c(2, 4, 6, 7, 9))\r\n# pca_barplot <- barplot(BankChurners_pca$eig[, 1], \r\n#                        main = \"Eigenvalues\", \r\n#                        names.arg = 1:nrow(BankChurners_pca$eig))\r\n# summary(BankChurners_pca)\r\n# BankChurners_pca$eig\r\n# BankChurners_pca$var\r\n\r\n# FactoMineR-FAMD\r\n# BankChurners2_rose_pca <- BankChurners2_rose\r\n# BankChurners_pca <- FAMD(BankChurners2_rose_pca, \r\n#      ncp = 16,\r\n#      sup.var = 2,\r\n#      graph = FALSE)\r\n# BankChurners_pca$eig\r\n# pca_barplot <- barplot(BankChurners_pca$eig[, 1], \r\n#                        main = \"Eigenvalues\", \r\n#                        names.arg = 1:nrow(BankChurners_pca$eig))\r\n\r\n\r\n\r\nData Split\r\n\r\n\r\nindex <- createDataPartition(BankChurners2_rose$Attrition_Flag, p = .6, list = FALSE, times = 1)\r\ntrain <- BankChurners2_rose[index,]\r\ntest <- BankChurners2_rose[-index,]\r\n\r\n\r\n\r\nModel-Training\r\n\r\n\r\nchurn_RF <- train(\r\n  form = factor(Attrition_Flag) ~.,\r\n  data = train,\r\n  trControl = trainControl(method = \"cv\",\r\n                           number = 10,\r\n                           classProbs = TRUE),\r\n  method = \"rf\",\r\n  tuneLength = 6\r\n)\r\n\r\n\r\n\r\nModel-Testing\r\n\r\n\r\nchurn_RF_pred <- predict(churn_RF, test, type = \"prob\")\r\nchurn_RF_test_pred <- cbind(churn_RF_pred, test)\r\nchurn_RF_test_pred <- churn_RF_test_pred %>% \r\n  mutate(prediction = if_else(ExistingCustomer > AttritedCustomer, \"ExistingCustomer\", \"AttritedCustomer\"))\r\ntable(churn_RF_test_pred$prediction)\r\n\r\n\r\n\r\nAttritedCustomer ExistingCustomer \r\n            2114             1936 \r\n\r\nPerformance\r\n\r\n\r\nconfusionMatrix(factor(churn_RF_test_pred$prediction), factor(churn_RF_test_pred$Attrition_Flag))\r\n\r\n\r\nConfusion Matrix and Statistics\r\n\r\n                  Reference\r\nPrediction         ExistingCustomer AttritedCustomer\r\n  ExistingCustomer             1623              313\r\n  AttritedCustomer              381             1733\r\n                                          \r\n               Accuracy : 0.8286          \r\n                 95% CI : (0.8167, 0.8401)\r\n    No Information Rate : 0.5052          \r\n    P-Value [Acc > NIR] : < 2e-16         \r\n                                          \r\n                  Kappa : 0.6571          \r\n                                          \r\n Mcnemar's Test P-Value : 0.01098         \r\n                                          \r\n            Sensitivity : 0.8099          \r\n            Specificity : 0.8470          \r\n         Pos Pred Value : 0.8383          \r\n         Neg Pred Value : 0.8198          \r\n             Prevalence : 0.4948          \r\n         Detection Rate : 0.4007          \r\n   Detection Prevalence : 0.4780          \r\n      Balanced Accuracy : 0.8284          \r\n                                          \r\n       'Positive' Class : ExistingCustomer\r\n                                          \r\n\r\nvar_import <- varImp(churn_RF)$importance %>% \r\n  arrange(desc(Overall))\r\nkable(var_import)\r\n\r\n\r\n\r\nOverall\r\nTotal_Trans_Ct\r\n100.0000000\r\nTotal_Ct_Chng_Q4_Q1\r\n52.3440562\r\nTotal_Trans_Amt\r\n37.0456672\r\nTotal_Relationship_Count\r\n32.3691455\r\nMonths_Inactive_12_mon\r\n29.7206153\r\nTotal_Amt_Chng_Q4_Q1\r\n25.1475232\r\nContacts_Count_12_mon\r\n23.7543125\r\nTotal_Revolving_Bal\r\n16.0882548\r\nCustomer_Age\r\n15.9575731\r\nMonths_on_book\r\n15.6122966\r\nAvg_Open_To_Buy\r\n14.2400466\r\nIncome_Level\r\n13.9393870\r\nCredit_Limit\r\n13.7861466\r\nAvg_Utilization_Ratio\r\n13.7561393\r\nDependent_count\r\n13.5437120\r\nCLIENTNUM\r\n13.5401712\r\nGenderF\r\n3.7121192\r\nMarital_StatusSingle\r\n2.3351003\r\nEducation_LevelGraduate\r\n1.6852358\r\nEducation_LevelHigh School\r\n1.3652059\r\nCard_CategoryBlue\r\n1.3524077\r\nEducation_LevelCollege\r\n1.2210126\r\nEducation_LevelPost-Graduate\r\n1.1829780\r\nEducation_LevelDoctorate\r\n0.8623315\r\nMarital_StatusDivorced\r\n0.6848503\r\nCard_CategorySilver\r\n0.6653834\r\nCard_CategoryPlatinum\r\n0.0000000\r\n\r\nVI. Results\r\n\r\nResults:\r\n\r\n\r\n\r\n\r\n",
      "last_modified": "2021-11-17T14:24:15-06:00"
    },
    {
      "path": "index.html",
      "title": "Siwei Jiang",
      "author": [],
      "contents": "\r\n\r\n          \r\n          \r\n          Siwei Jiang\r\n          \r\n          \r\n          \r\n          \r\n          \r\n          \r\n          \r\n          \r\n          \r\n          \r\n          \r\n          \r\n          \r\n          \r\n          \r\n          \r\n          \r\n          Resume\r\n          \r\n          \r\n          Project\r\n           \r\n          â–¾\r\n          \r\n          \r\n          Rsquared\r\n          Machine Learning\r\n          Final Project\r\n          \r\n          \r\n          â˜°\r\n          \r\n          \r\n      \r\n        \r\n          \r\n            Siwei Jiang\r\n          \r\n          \r\n            \r\n              Change me later:)\r\n              \r\n              \r\n              h1, body {\r\n                color: #bdaeea;\r\n              }\r\n              \r\n            \r\n              Change me later:)\r\n              \r\n              \r\n              h1, body {\r\n                color: #bdaeea;\r\n              }\r\n              \r\n          \r\n\r\n          \r\n            \r\n              \r\n                  \r\n                    \r\n                      Github\r\n                    \r\n                  \r\n                \r\n                                \r\n                  \r\n                    \r\n                      Email\r\n                    \r\n                  \r\n                \r\n                                \r\n                  \r\n                    \r\n                      Blog\r\n                    \r\n                  \r\n                \r\n                              \r\n          \r\n\r\n          \r\n            \r\n              \r\n                                \r\n                  \r\n                    Github\r\n                  \r\n                \r\n                                \r\n                  \r\n                    Email\r\n                  \r\n                \r\n                                \r\n                  \r\n                    Blog\r\n                  \r\n                \r\n                              \r\n            \r\n          \r\n        \r\n      \r\n    \r\n\r\n    \r\n    \r\n    ",
      "last_modified": "2021-11-17T11:04:02-06:00"
    },
    {
      "path": "ml.html",
      "title": "ML",
      "author": [],
      "contents": "\r\nuse hw\r\ndescribe the model\r\ndescribe the process and results\r\ncode\r\n\r\n\r\n\r\n",
      "last_modified": "2021-11-17T11:04:04-06:00"
    },
    {
      "path": "resume.html",
      "title": "RÃ©sumÃ©",
      "author": [],
      "contents": "\r\n\r\n\r\na {n  \r\n  text-decoration: none; \r\n  color: rgba(121, 205, 205);\r\n}\r\na:hover {\r\n  color: rgba(255, 182, 255, 0.8);\r\n}\r\nul {\r\n  list-style: none; /* Remove list bullets bcz don't know how to change color */\r\n  padding: 0;\r\n  margin: 0;\r\n}\r\nli::before {\r\n  content: \"â€¢\"; \r\n  color: \"#5A59A3\";\r\n}\r\nh1 {\r\n  font-family: candara; /* title */\r\n  font-style: italic;\r\n}\r\nh2 {\r\n  font-family: candara;\r\n  font-style: italic;\r\n  color: rgba(134, 35, 141, 0.72);\r\n}\r\np code,\r\nli code {\r\n    line-height: 2.0; \r\n    white-space: nowrap;\r\n}\r\nbody {\r\n    width: 100vw;\r\n    height: 100vh;\r\n    margin: 0;\r\n    background: linear-gradient(\r\n        130deg, \r\n        #5A59A3, \r\n        #C66060\r\n    ) no-repeat  fixed;\r\n}\r\n\r\nEducation\r\nBachelor of Business in Accounting: Adkerson School of Accountancy May 2020\r\nMinor in Business Analytics\r\nMississippi State University, Mississippi State, MS\r\nWorking Experience\r\nÂ Â Private Accountant for a Fire Protection Service CompanyFall 2014 â€“ Fall 2015\r\nÂ Â Hunan, China\r\nÂ Â Â Â Â Â Â Â Â Â Â Â Â Â Journal entries and general ledger preparation\r\nÂ Â Â Â Â Â Â Â Â Â Â Â Â Â Bank statements and reconciliation\r\nÂ Â Â Â Â Â Â Â Â Â Â Â Â Â Examining expenses submitted by the construction manager\r\nÂ Â Â Â Â Â Â Â Â Â Â Â Â Â Taxation preparation during tax season\r\nCourse Group Projects\r\nÂ Â PowerPoint and social platforms: Marketing strategy plans for a drone startup\r\nÂ Â Excel and Word: Prepared consolidated financial reports for company acquisition\r\nÂ Â R: Bankruptcy analysis. Methods: holdout validation in discriminant analysis and logistic Â Â Â Â Â Â Â  regression analysis, and cross-validation in decision tree analysis\r\nComputer Skills\r\nÂ Â Advanced in Microsoft Office including Excel, Word, and PowerPoint\r\nÂ Â Developing knowledge in Access, R, SAS, and Tableau\r\nVolunteer Work\r\nÂ Â West Point Clay County Animal Shelter\r\nÂ Â MSU Community Garden\r\nÂ Â Starkville Community Farmers Market\r\nÂ Â The Salvation Army\r\n\r\n\r\n\r\n",
      "last_modified": "2021-11-17T11:04:05-06:00"
    },
    {
      "path": "rsquared.html",
      "title": "R-squared",
      "author": [],
      "date": "`r Sys.Date()`",
      "contents": "\r\nTime to blow RSquared up1 ðŸ’¥\r\nR-squared is a statistic that often accompanies regression output. It ranges in value from 0 to 1 and is usually interpreted as summarizing the percent of variation in the response that the regression model explains. So an R-squared of 0.65 might mean that the model explains about 65% of the variation in our dependent variable. Given this logic, we prefer our regression models have a high R-squared.\r\nIn R, we typically get R-squared by calling the summary function on a model object. Hereâ€™s a quick example using simulated data:\r\n\r\n\r\n# independent variable\r\nx <- 1:20 \r\n# for reproducibility\r\nset.seed(1) \r\n# dependent variable; function of x with random error\r\ny <- 2 + 0.5*x + rnorm(20,0,3) \r\n# simple linear regression\r\nmod <- lm(y~x)\r\n# request just the r-squared value\r\nsummary(mod)$r.squared          \r\n\r\n\r\n[1] 0.6026682\r\n\r\nOne way to express R-squared is as the sum of squared fitted-value deviations divided by the sum of squared original-value deviations:\r\n\\[\r\nR^{2} =  \\frac{\\sum (\\hat{y} â€“ \\bar{\\hat{y}})^{2}}{\\sum (y â€“ \\bar{y})^{2}}\r\n\\]\r\nWe can calculate it directly using our model object like so:\r\n\r\n\r\n# extract fitted (or predicted) values from model\r\nf <- mod$fitted.values\r\n# sum of squared fitted-value deviations\r\nmss <- sum((f - mean(f))^2)\r\n# sum of squared original-value deviations\r\ntss <- sum((y - mean(y))^2)\r\n# r-squared\r\nmss/tss                      \r\n\r\n\r\n[1] 0.6026682\r\n\r\n1. R-squared does not measure goodness of fit. It can be arbitrarily low when the model is completely correct. By making\\(Ïƒ^2\\) large, we drive R-squared towards 0, even when every assumption of the simple linear regression model is correct in every particular.\r\nWhat is \\(Ïƒ^2\\)? When we perform linear regression, we assume our model almost predicts our dependent variable. The difference between â€œalmostâ€ and â€œexactâ€ is assumed to be a draw from a Normal distribution with mean 0 and some variance we call \\(Ïƒ^2\\).\r\nThis statement is easy enough to demonstrate. The way we do it here is to create a function that (1) generates data meeting the assumptions of simple linear regression (independent observations, normally distributed errors with constant variance), (2) fits a simple linear model to the data, and (3) reports the R-squared. Notice the only parameter for sake of simplicity is sigma. We then â€œapplyâ€ this function to a series of increasing \\(Ïƒ\\) values and plot the results.\r\n\r\n\r\nr2.0 <- function(sig){\r\n  # our predictor\r\n  x <- seq(1,10,length.out = 100)   \r\n  # our response; a function of x plus some random noise\r\n  y <- 2 + 1.2*x + rnorm(100,0,sd = sig) \r\n  # print the R-squared value\r\n  summary(lm(y ~ x))$r.squared          \r\n}\r\nsigmas <- seq(0.5,20,length.out = 20)\r\n # apply our function to a series of sigma values\r\nrout <- sapply(sigmas, r2.0)            \r\nplot(rout ~ sigmas, type=\"b\")\r\n\r\n\r\n\r\n\r\nR-squared tanks hard with increasing sigma, even though the model is completely correct in every respect.\r\nR-squared can be arbitrarily close to 1 when the model is totally wrong.\r\nThe point being made is that R-squared does not measure goodness of fit.\r\n\r\n\r\nset.seed(1)\r\n# our predictor is data from an exponential distribution\r\nx <- rexp(50,rate=0.005)\r\n# non-linear data generation\r\ny <- (x-1)^2 * runif(50, min=0.8, max=1.2) \r\n# clearly non-linear\r\nplot(x,y)             \r\n\r\n\r\n\r\n\r\n\r\n\r\nsummary(lm(y ~ x))$r.squared\r\n\r\n\r\n[1] 0.8485146\r\n\r\nItâ€™s very high at about 0.85, but the model is completely wrong. Using R-squared to justify the â€œgoodnessâ€ of our model in this instance would be a mistake. Hopefully one would plot the data first and recognize that a simple linear regression in this case would be inappropriate.\r\n3. R-squared says nothing about prediction error, even with \\(Ïƒ^2\\) exactly the same, and no change in the coefficients. R-squared can be anywhere between 0 and 1 just by changing the range of X. Weâ€™re better off using Mean Square Error (MSE) as a measure of prediction error.\r\nMSE is basically the fitted y values minus the observed y values, squared, then summed, and then divided by the number of observations.\r\nLetâ€™s demonstrate this statement by first generating data that meets all simple linear regression assumptions and then regressing y on x to assess both R-squared and MSE.\r\n\r\n\r\nx <- seq(1,10,length.out = 100)\r\nset.seed(1)\r\ny <- 2 + 1.2*x + rnorm(100,0,sd = 0.9)\r\nmod1 <- lm(y ~ x)\r\nsummary(mod1)$r.squared\r\n\r\n\r\n[1] 0.9383379\r\n\r\n# Mean squared error\r\nsum((fitted(mod1) - y)^2)/100\r\n\r\n\r\n[1] 0.6468052\r\n\r\nNow repeat the above code, but this time with a different range of x. Leave everything else the same:\r\n\r\n\r\n # new range of x\r\nx <- seq(1,2,length.out = 100)      \r\nset.seed(1)\r\ny <- 2 + 1.2*x + rnorm(100,0,sd = 0.9)\r\nmod1 <- lm(y ~ x)\r\nsummary(mod1)$r.squared\r\n\r\n\r\n[1] 0.1502448\r\n\r\n# Mean squared error\r\nsum((fitted(mod1) - y)^2)/100        \r\n\r\n\r\n[1] 0.6468052\r\n\r\nThe R-squared falls from 0.94 to 0.15 but the MSE remains the same. In other words the predictive ability is the same for both data sets, but the R-squared would lead you to believe the first example somehow had a model with more predictive power.\r\nR-squared can easily go down when the model assumptions are better fulfilled.\r\nLetâ€™s examine this by generating data that would benefit from transformation. Notice the R code below is very much like our previous efforts but now we exponentiate our y variable.\r\n\r\n\r\nx <- seq(1,2,length.out = 100)\r\nset.seed(1)\r\ny <- exp(-2 - 0.09*x + rnorm(100,0,sd = 2.5))\r\nsummary(lm(y ~ x))$r.squared\r\n\r\n\r\n[1] 0.003281718\r\n\r\nplot(lm(y ~ x), which=3)\r\n\r\n\r\n\r\n\r\nR-squared is very low and our residuals vs.Â fitted plot reveals outliers and non-constant variance. A common fix for this is to log transform the data. Letâ€™s try that and see what happens:\r\n\r\n\r\nplot(lm(log(y)~x),which = 3) \r\n\r\n\r\n\r\n\r\nThe diagnostic plot looks much better. Our assumption of constant variance appears to be met. But look at the R-squared:\r\n\r\n\r\nsummary(lm(log(y)~x))$r.squared \r\n\r\n\r\n[1] 0.0006921086\r\n\r\nItâ€™s even lower! This is an extreme case and it doesnâ€™t always happen like this. In fact, a log transformation will usually produce an increase in R-squared. But as just demonstrated, assumptions that are better fulfilled donâ€™t always lead to higher R-squared.\r\nIt is very common to say that R-squared is â€œthe fraction of variance explainedâ€ by the regression. \\[Yet\\] if we regressed X on Y, weâ€™d get exactly the same R-squared. This in itself should be enough to show that a high R-squared says nothing about explaining one variable by another.\r\nThis is the easiest statement to demonstrate:\r\n\r\n\r\nx <- seq(1,10,length.out = 100)\r\ny <- 2 + 1.2*x + rnorm(100,0,sd = 2)\r\nsummary(lm(y ~ x))$r.squared\r\n\r\n\r\n[1] 0.737738\r\n\r\nsummary(lm(x ~ y))$r.squared\r\n\r\n\r\n[1] 0.737738\r\n\r\nDoes x explain y, or does y explain x? Are we saying â€œexplainâ€ to dance around the word â€œcauseâ€? In a simple scenario with two variables such as this, R-squared is simply the square of the correlation between x and y:\r\n\r\n\r\nall.equal(cor(x,y)^2, summary(lm(x ~ y))$r.squared, summary(lm(y ~ x))$r.squared)\r\n\r\n\r\n[1] TRUE\r\n\r\nLetâ€™s recap:\r\nR-squared does not measure goodness of fit.\r\nR-squared does not measure predictive error.\r\nR-squared does not necessarily increase when assumptions are better satisfied.\r\nR-squared does not measure how one variable explains another.\r\n\r\nhttps://data.library.virginia.edu/is-r-squared-useless/â†©ï¸Ž\r\n",
      "last_modified": "2021-11-17T11:04:12-06:00"
    }
  ],
  "collections": ["posts/posts.json"]
}
