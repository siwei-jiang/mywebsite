---
title: "Customer Churn Prediction"
description: |
  This report is the final project for my accounitng analysis class, and the codes are borrowed from my professor, Dr. Hunt's [course website](https://professor-hunt.github.io/ACC8143/){target="_blank"}.
date: "`r Sys.Date()`"
output: 
  distill::distill_article:
    toc: true
    toc_depth: 3
    code_folding: true
---

```{r setup, set.seed(0), include=FALSE, echo=FALSE}
knitr::opts_chunk$set(error = TRUE, cache = TRUE, warning = FALSE)
library(readr)
library(readxl)
library(gdata)
library(GGally)
library(ggplot2)
library(caret)
library(dplyr)
library(knitr)
library(hrbrthemes)
library(gplots)
library(xaringan)
library(xaringanExtra)
library(sknifedatar)
library(kableExtra)
library(gbm)
library(fastAdaboost)
library(VIM)
xaringanExtra::use_panelset()
xaringanExtra::style_panelset_tabs()
BankChurners <- read_csv("data/BankChurners.csv", 
                         col_types = cols(CLIENTNUM = col_number(), 
                                          Attrition_Flag = col_factor(levels = c()), 
                                          Customer_Age = col_number(), Gender = col_factor(levels = c()), 
                                          Dependent_count = col_number(), Education_Level = col_factor(levels = c()), 
                                          Marital_Status = col_factor(levels = c()), 
                                          Income_Level = col_number(), Income_Category = col_factor(levels = c()), 
                                          Card_Category = col_factor(levels = c()), 
                                          Months_on_book = col_number(), Total_Relationship_Count = col_number(), 
                                          Months_Inactive_12_mon = col_number(), 
                                          Contacts_Count_12_mon = col_number(), 
                                          Credit_Limit = col_number(), Total_Revolving_Bal = col_number(), 
                                          Avg_Open_To_Buy = col_number(), Total_Amt_Chng_Q4_Q1 = col_number(), 
                                          Total_Trans_Amt = col_number(), Total_Trans_Ct = col_number(), 
                                          Total_Ct_Chng_Q4_Q1 = col_number(), 
                                          Avg_Utilization_Ratio = col_number(), 
                                          Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_1 = col_number(), 
                                          Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_2 = col_number()), 
                         na = "0")
save(BankChurners, file = "BankChurners.Rda")
BankChurners <- BankChurners[, -c(1, 9, 23, 24)]
BankChurners$Education_Level <- factor(BankChurners$Education_Level, 
                                       levels = c("Uneducated", "High School", "College", "Graduate", "Post-Graduate", "Doctorate"))
IncomeLevel <- read_excel("data/IncomeLevel.xlsx")
save(IncomeLevel, file = "IncomeLevel.Rda")
# View(BankChurners)
# summary(BankChurners)
```

## I. About the data set
I obtained this data set from Kaggle^[https://www.kaggle.com/sakshigoyal7/credit-card-customers], and Sakshi Goyal uploaded it in 2020. This data set is about a üè¶bank's customer churn issue. The managerüíº of the bank is interested in predicting which customer will leave this banküí∏. By doing so, the bank can target those customers with special products and services to increase their satisfaction and customer retentionüåû.  
  
It contains 10,127 observations and 23 variables. Because I had some issues with Income Category values, so I add another column that used a scale of 1 through 5 to represent each income category in Excel before importing it to the Rstudio. My data analyses are based on 20 variables, which excluded the Client Number and two Naive Bayes Classifiersüò∂.  
  
The purpose of this project is to predict churned customers, so I use recall as an important model fit measurementüìê. I also include accuracy and kappa as measurements of model fitness. I choose kappa because customer churn is only around 16% of total customers. Although some studies state kappa is not a good measure for classification model^[https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0222916], I think it is good enough for this projectüòé I include a comparison tableüìä in the last section, Result for each models' performance. 

## II. Descriptive Stats
This data set has 9,664 missing values, which is 4.77% of the 202,540 total values. Most of those missing values are belong to categorical variables.  

After using kNN to replace all the missing data by calculating their nearest 10 neighbors üè†üè° values, the new data set contains 0Ô∏è‚É£ missing values. The distribution of the target variable is not balanced‚öñÔ∏è, as attrited customers only represent 16% of the total customers. Because of the potential overfitting issue, I used caret's built-in function SMOTE oversampling to overcome this issue, and it did improved the model performance. Below are some visualizations that are used for gaining some insights of the underlying data.


<aside>
```{r NA}
BankChurners1 <- unknownToNA(BankChurners, unknown = c("", "Unknown"))
# summary(BankChurners1)
NA_cnt <- table(is.na(BankChurners1))
NA_pct <- prop.table(NA_cnt)
cbind(NA_cnt, NA_pct)

# replace all missing values 
BankChurners2 <- VIM::kNN(BankChurners1, 
                          variable = c("Dependent_count", "Education_Level", 
                                       "Marital_Status", "Income_Level", 
                                       "Months_Inactive_12_mon", "Contacts_Count_12_mon",
                                       "Total_Revolving_Bal", "Total_Amt_Chng_Q4_Q1",
                                       "Total_Ct_Chng_Q4_Q1", "Avg_Utilization_Ratio"),
                          k = 10)
BankChurners2 <- BankChurners2[, -c(21:30)]
table(is.na(BankChurners2))

NA_cnt <- table(BankChurners2$Attrition_Flag)
NA_pct <- prop.table(counts)
cbind(NA_cnt, NA_pct)
```
</aside>  



::: panelset
::: panel
[Transaction Count]{.panel-name} Attrited customer group has less variability and less spread regarding the total number of transactions. We can see that 50% of customer churns have less than 50 transaction counts and no of those customers have more than 100 transactions.
       
```{r plot1}
ggplot(data = BankChurners1, 
       mapping = aes(x = Attrition_Flag, 
                     y = Total_Trans_Ct,
                     fill = Attrition_Flag)) +
  labs(title = "Boxplot: Total Transaction Count",
       tag = "Fig. 1") +
  geom_boxplot(alpha = .3) +
  theme(legend.position = "none") +
  scale_fill_brewer(palette = "Dark2")
```
:::

::: panel
[Transaction Amtüíµ]{.panel-name} Both existing and attrited Customers are right-skewed due to outliers regarding their total transaction amounts.

```{r plot2}
ggplot(data = BankChurners1, 
       mapping = aes(x = Attrition_Flag, 
                     y = Total_Trans_Amt,
                     fill = Attrition_Flag)) +
  labs(title = "Boxplot: Total Transaction Amount",
       tag = "Fig. 2") +
  geom_boxplot(alpha = .3) +
  theme(legend.position = "none") +
  scale_fill_brewer(palette = "Accent")
```
:::

::: panel
[üå†Credit Limit]{.panel-name} The majority of attrited customers have lower than $5000 credit.

```{r plot4}
ggplot(data = BankChurners1,
       mapping = aes(x = Credit_Limit,
                     fill = Attrition_Flag)) +
  geom_histogram(color = "#e9ecef", 
                 alpha = 1,
                 position = "stack") +
  scale_fill_manual(values=c("#adb8ff", "#e8b5ff")) +
  theme_ipsum() +
  labs(title = "Histogram: Credit Limit vs Attrition",
       tag = "Fig. 4")
```
:::

::: panel
[Income vs Gender]{.panel-name} Females' income levels are concentrated at low levels such as levels 1 and 2 while males have much higher income level distributions.

```{r plot3}
ggplot(data = BankChurners1,
       mapping = aes(x = Income_Level,
                     group = Gender,
                     fill = Gender)) +
  geom_density(adjust = 1.5, alpha = .4) +
  theme_ipsum() +
  labs(title = "Density Plot: Gender vs Income Level",
       tag = "Fig. 3") +
  scale_color_manual(values=c("M"="blue", "F"="pink")) # not working
```

<aside>
```{r income_level}
kable(IncomeLevel)
```
</aside>
:::
:::

## III. The Models
The three models used for this project are as follow:
  
1. Random Forestüå≥:  Decision trees are sensitive to changes, introducing randomness to each split of trees, reduces the over-fitting issue. Also, random forest uses bagging method to make decisions based on a majority vote from each individual tree.
2. Gradient Boosting Treeüå≤:  Gradient boosting is normally betterüí™ than random forest due to its arbitrary cost functions for classification purpose^[https://en.wikipedia.org/wiki/Gradient_boosting].
3. Neural NetworküîÆ: Neural network is very effective and efficient in making inferences and detecting patterns from complex data sets. It uses the input information to optimize the weight of those inputs and then generates outputs. It also minimizes the errors ‚ùå of those outputs to improve those processed inputs until the errors become small enoughüîÑ. The final result is based on minimized errors‚ùì

## IV. The Process
I split the data set into the training set and testing set based on 4 different ratios, and the 7:3 ratio has the best result, so I use this ratio for rest of models: üå≥random forest, gradient boosting treeüå≤, and üí´neural networküîÆ to trainüöã each data set. After importing the original data set, I use the kNN function with k = 10 to replace those missing values. For comparison, I replace missing values only in the training set and leave the testing set as it is. Both random forest and gradient boosting trees have better performance in terms of recall after re-sampling in the training sets. Neural network model turns out to have the worst performance. I am going to use SOMTE for the neural network model just to see the comparison. It does improved the recall quit bit even though the accuracy decreased a bit. I will update the comparsion table and the result description late. 

## V. Code 

### i. üéÑRandom Forest Total NA‚≠ê
> Original data set with Total Missing Value Replaced  

I use 4 different ratio to split the data set into a training set and a testing set. The 7:3 ratio has the best performance. The model has 0.9661 of accuracyüéØ and 0.8683 in kappa, this huge drop probably is due to the imbalanced distribution of the attrition. The recall is 0.8381, which means that this model will misclassify 2 attrited customers of every 10 customers as existing customers.     

```{r data_split}
comparison <- matrix(c(0.9599, 0.8437, 0.8155, 0.9590, 0.8406, 0.8154, 0.9661, 0.8683, 0.8381, 0.9580, 0.8359, 0.8062),
                    ncol = 3, byrow = TRUE)
colnames(comparison) <- c("Accuracy", "Kappa", "Recall")
rownames(comparison) <- c("5:5", "6:4", "7:3", "8:2")
comparison <- as.data.frame.matrix(comparison) 
kable(comparison) %>% 
  row_spec(3, color = "white", background = "#bdaeea")
```

::: panelset
::: panel
[5:5]{.panel-name} Data split in 5:5 ratio.

```{r RF11}
# data split
index <- createDataPartition(BankChurners2$Attrition_Flag, 
                             p = .5, list = FALSE, times = 1)
train <- BankChurners2[index,]
test <- BankChurners2[-index,]

# train1
churn_RF1 <- train(
  form = factor(Attrition_Flag) ~.,
  data = train,
  trControl = trainControl(method = "cv",
                           number = 10,
                           classProbs = TRUE),
  method = "rf",
  tuneLength = 10
)
# churn_RF1

# feature importance
var_imp1 <- varImp(churn_RF1)$importance %>% 
  arrange(desc(Overall))
kable(head(var_imp1)) 

ggplot(var_imp1, aes(x = reorder(rownames(var_imp1), Overall), y = Overall)) +
  geom_point(color = "plum1", size = 6, alpha = 1) +
  geom_segment(aes(x = rownames(var_imp1), xend = rownames(var_imp1), 
                   y = 0, yend = Overall), color = "skyblue") +
  xlab("Variable") +
  ylab("Overall Importance") +
  theme_light() +  
  coord_flip()

# test1
churn_RF_pred1 <- predict(churn_RF1, test, type = "prob")
churn_RF_test_pred1 <- cbind(churn_RF_pred1, test)
churn_RF_test_pred1 <- churn_RF_test_pred1 %>% 
  mutate(prediction = if_else(AttritedCustomer > ExistingCustomer, 
                              "AttritedCustomer", "ExistingCustomer"))
table(churn_RF_test_pred1$prediction)

# result1
churn_matrix1 <- confusionMatrix(factor(churn_RF_test_pred1$prediction), 
                                 factor(churn_RF_test_pred1$Attrition_Flag), 
                                 positive = "AttritedCustomer")
churn_matrix1
# Accuracy : 0.9632  Kappa : 0.8586 Sensitivity : 0.8431

ggplot(as.data.frame(churn_matrix1$table)) +
  geom_raster(aes(x = Reference, y = Prediction, fill = Freq)) +
  geom_text(aes(x = Reference, y = Prediction, label = Freq)) +
  scale_fill_gradient2(low = "darkred", high = "darkgreen", 
                       na.value = "gray", name = "Freq") +
  scale_x_discrete(name = "Actual Class") +
  scale_y_discrete(name = "Predicted Class") +
  ggtitle("Random Forest Confusion Matrix") +
  theme(plot.title = element_text(hjust = .5, size = 10, face = "bold"))
```
:::
 
::: panel
[6:4]{.panel-name} Data split in 6:4 ratio. 

```{r RF12}
# data split
index <- createDataPartition(BankChurners2$Attrition_Flag, 
                             p = .6, list = FALSE, times = 1)
train <- BankChurners2[index,]
test <- BankChurners2[-index,]

# train1
churn_RF1 <- train(
  form = factor(Attrition_Flag) ~.,
  data = train,
  trControl = trainControl(method = "cv",
                           number = 10,
                           classProbs = TRUE),
  method = "rf",
  tuneLength = 10
)
# churn_RF1

# test1
churn_RF_pred1 <- predict(churn_RF1, test, type = "prob")
churn_RF_test_pred1 <- cbind(churn_RF_pred1, test)
churn_RF_test_pred1 <- churn_RF_test_pred1 %>% 
  mutate(prediction = if_else(AttritedCustomer > ExistingCustomer, 
                              "AttritedCustomer", "ExistingCustomer"))

# result1
churn_matrix1 <- confusionMatrix(factor(churn_RF_test_pred1$prediction), 
                                 factor(churn_RF_test_pred1$Attrition_Flag), 
                                 positive = "AttritedCustomer")
churn_matrix1
# Accuracy : 0.9632  Kappa : 0.8586 Sensitivity : 0.8431
```
:::

::: panel
[7:3]{.panel-name} Data split in 7:3 ratio. 

```{r RF13}
# data split
index <- createDataPartition(BankChurners2$Attrition_Flag, 
                             p = .7, list = FALSE, times = 1)
train <- BankChurners2[index,]
test <- BankChurners2[-index,]

# train1
churn_RF1 <- train(
  form = factor(Attrition_Flag) ~.,
  data = train,
  trControl = trainControl(method = "cv",
                           number = 10,
                           classProbs = TRUE),
  method = "rf",
  tuneLength = 10
)
# churn_RF1

# test1
churn_RF_pred1 <- predict(churn_RF1, test, type = "prob")
churn_RF_test_pred1 <- cbind(churn_RF_pred1, test)
churn_RF_test_pred1 <- churn_RF_test_pred1 %>% 
  mutate(prediction = if_else(AttritedCustomer > ExistingCustomer, 
                              "AttritedCustomer", "ExistingCustomer"))
table(churn_RF_test_pred1$prediction)

# result1
churn_matrix1 <- confusionMatrix(factor(churn_RF_test_pred1$prediction), 
                                 factor(churn_RF_test_pred1$Attrition_Flag), 
                                 positive = "AttritedCustomer")
churn_matrix1
```
:::

::: panel
[8:2]{.panel-name} Data split in 8:2 ratio. 

```{r RF14}
# data split
index <- createDataPartition(BankChurners2$Attrition_Flag, 
                             p = .8, list = FALSE, times = 1)
train <- BankChurners2[index,]
test <- BankChurners2[-index,]

# train1
churn_RF1 <- train(
  form = factor(Attrition_Flag) ~.,
  data = train,
  trControl = trainControl(method = "cv",
                           number = 10,
                           classProbs = TRUE),
  method = "rf",
  tuneLength = 10
)

# test1
churn_RF_pred1 <- predict(churn_RF1, test, type = "prob")
churn_RF_test_pred1 <- cbind(churn_RF_pred1, test)
churn_RF_test_pred1 <- churn_RF_test_pred1 %>% 
  mutate(prediction = if_else(AttritedCustomer > ExistingCustomer, 
                              "AttritedCustomer", "ExistingCustomer"))

# result1
churn_matrix1 <- confusionMatrix(factor(churn_RF_test_pred1$prediction), 
                                 factor(churn_RF_test_pred1$Attrition_Flag), 
                                 positive = "AttritedCustomer")
churn_matrix1
```
:::
:::

### ii. üå¥Random Forest NA & VarüçÉ 
> data set with 14 variables and Total Missing Value Replaced  

Education levelüéì, marital statusüíë, card categoryüí≥, income levelüí∞, dependent countüë∂, and genderüë¶ üë© are the least important variables that used for the final prediction. After dropping those 6 variables, the model performance decreased a little bit compared to the previous model's.  

```{r RF1_drop_var}
# data split drop 6 var
BankChurners_drop_var <- BankChurners2[-c(3:8)]
index_var <- createDataPartition(BankChurners_drop_var$Attrition_Flag, 
                                 p = .7, list = FALSE, times = 1)
train_var <- BankChurners_drop_var[index,]
test_var <- BankChurners_drop_var[-index,]

# train drop 6 var
churn_RF_var <- train(
  form = factor(Attrition_Flag) ~.,
  data = train_var,
  trControl = trainControl(method = "cv",
                           number = 10,
                           classProbs = TRUE),
  method = "rf",
  tuneLength = 10
)
# churn_RF_var

# test drop 6 var
churn_RF_pred_var <- predict(churn_RF_var, test_var, type = "prob")
churn_RF_test_pred_var <- cbind(churn_RF_pred_var, test_var)
churn_RF_test_pred_var <- churn_RF_test_pred_var %>% 
  mutate(prediction = if_else(AttritedCustomer > ExistingCustomer, 
                              "AttritedCustomer", "ExistingCustomer"))
table(churn_RF_test_pred_var$prediction)

# result drop 6 var
churn_matrix_var <- confusionMatrix(factor(churn_RF_test_pred_var$prediction), 
                                    factor(churn_RF_test_pred_var$Attrition_Flag), 
                                    positive = "AttritedCustomer")
churn_matrix_var
```

### iii. Random Forest Train NA‚ú®
> Original data set with Training Set Missing Value Replaced  

Only replaced missing values in the training set. The model has 0.9575 in accuracy and 0.8341 in kappa, which improved üìà compared to the previous model. The recall is 0.8062, which means that this model will misclassify 1 attrited customer of every 10 customers as existing customers.

```{r RF2}
# data split
index2 <- createDataPartition(BankChurners1$Attrition_Flag, 
                              p = .7, list = FALSE, times = 1)
train2 <- BankChurners1[index,]
test2 <- BankChurners1[-index,]
table(is.na(train2))

# replace missing values in the training set
train2 <- VIM::kNN(train2, 
                   variable = c("Dependent_count", "Education_Level", 
                                "Marital_Status", "Income_Level", 
                                "Months_Inactive_12_mon", "Contacts_Count_12_mon",
                                "Total_Revolving_Bal", "Total_Amt_Chng_Q4_Q1",
                                "Total_Ct_Chng_Q4_Q1", "Avg_Utilization_Ratio"),
                   k = 10)
# summary(train2)
train2 <- train2[, -c(21:30)]
table(is.na(train2))

# train2
churn_RF2 <- train(
  form = factor(Attrition_Flag) ~.,
  data = train2,
  trControl = trainControl(method = "cv",
                           number = 10,
                           classProbs = TRUE),
  method = "rf",
  tuneLength = 10
)
# churn_RF2

# test1
churn_RF_pred2 <- predict(churn_RF2, test, type = "prob")
churn_RF_test_pred2 <- cbind(churn_RF_pred2, test)
churn_RF_test_pred2 <- churn_RF_test_pred2 %>% 
  mutate(prediction = if_else(AttritedCustomer > ExistingCustomer, 
                              "AttritedCustomer", "ExistingCustomer"))
table(churn_RF_test_pred2$prediction)

#result1
churn_matrix2 <- confusionMatrix(factor(churn_RF_test_pred2$prediction), 
                                 factor(churn_RF_test_pred2$Attrition_Flag), 
                                 positive = "AttritedCustomer")
churn_matrix2

ggplot(as.data.frame(churn_matrix2$table)) +
  geom_raster(aes(x = Reference, y = Prediction, fill = Freq)) +
  geom_text(aes(x = Reference, y = Prediction, label = Freq)) +
  scale_fill_gradient2(low = "darkred", high = "plum1", 
                       na.value = "gray", name = "Freq") +
  scale_x_discrete(name = "Actual Class") +
  scale_y_discrete(name = "Predicted Class") +
  ggtitle("Random Forest Confusion Matrix") +
  theme(plot.title = element_text(hjust = .5, size = 10, face = "bold"))
```


### iv. Random Forest Total NA & SMOTE‚ùÑÔ∏è
> Original data set with Total Missing Value Replaced and Resampling in the Training Set  

The model has 0.9549 of accuracy and 0.8423 in kappa, which is similar to the previous model. The recall is 0.9344, the highest score among other models. 

```{r RF3}
# data split
index <- createDataPartition(BankChurners2$Attrition_Flag, 
                             p = .7, list = FALSE, times = 1)
train <- BankChurners2[index,]
test <- BankChurners2[-index,]

# train3
churn_RF3 <- train(
  form = factor(Attrition_Flag) ~.,
  data = train,
  trControl = trainControl(method = "cv",
                           number = 10,
                           classProbs = TRUE,
                           sampling = "smote"),
  method = "rf",
  tuneLength = 10
)
# churn_RF3

# feature importance
var_imp3 <- varImp(churn_RF3)$importance %>% 
  arrange(desc(Overall))
kable(head(var_imp3))

ggplot(var_imp3, aes(x = reorder(rownames(var_imp3), Overall), y = Overall)) +
  geom_point(color = "plum1", size = 6, alpha = 1) +
  geom_segment(aes(x = rownames(var_imp3), xend = rownames(var_imp3), 
                   y = 0, yend = Overall), color = "skyblue") +
  xlab("Variable") +
  ylab("Overall Importance") +
  theme_light() +
  coord_flip()

# test3
churn_RF_pred3 <- predict(churn_RF3, test, type = "prob")
churn_RF_test_pred3 <- cbind(churn_RF_pred3, test)
churn_RF_test_pred3 <- churn_RF_test_pred3 %>% 
  mutate(prediction = if_else(AttritedCustomer > ExistingCustomer, 
                              "AttritedCustomer", "ExistingCustomer"))
table(churn_RF_test_pred3$prediction)

# result3
churn_matrix3 <- confusionMatrix(factor(churn_RF_test_pred3$prediction), 
                                 factor(churn_RF_test_pred3$Attrition_Flag), 
                                 positive = "AttritedCustomer")
churn_matrix3

ggplot(as.data.frame(churn_matrix3$table)) +
  geom_raster(aes(x = Reference, y = Prediction, fill = Freq)) +
  geom_text(aes(x = Reference, y = Prediction, label = Freq)) +
  scale_fill_gradient2(low = "darkred", high = "powderblue", 
                       na.value = "gray", name = "Freq") +
  scale_x_discrete(name = "Actual Class") +
  scale_y_discrete(name = "Predicted Class") +
  ggtitle("Random Forest & SMOTE Confusino Matrix") +
  theme(plot.title = element_text(hjust = .5, size = 10, face = "bold"))
```


### v. Gradient Boosting Tree Total NA
> Original data set with Total Missing Value Replaced  

The model has 0.9681 of accuracy and 0.8781 in kappa. The recall is 0.8648 Gradient boosting tree has a better performance compared to the random forest under the same condition.            

```{r GBM1}
# data split
index <- createDataPartition(BankChurners2$Attrition_Flag, 
                             p = .7, list = FALSE, times = 1)
train <- BankChurners2[index,]
test <- BankChurners2[-index,]

# train4
churn_GBM1 <- train(
  form = factor(Attrition_Flag) ~.,
  data = train,
  trControl = trainControl(method = "cv",
                           number = 10,
                           classProbs = TRUE),
  method = "gbm",
  tuneLength = 10,
  verbose = FALSE
)
kable(churn_GBM1$bestTune)
plot(churn_GBM1)

# test4
churn_GBM_pred1 <- predict(churn_GBM1, test, type = "prob")
churn_GBM_test_pred1 <- cbind(churn_GBM_pred1, test)
churn_GBM_test_pred1 <- churn_GBM_test_pred1 %>% 
  mutate(prediction = if_else(AttritedCustomer > ExistingCustomer, 
                              "AttritedCustomer", "ExistingCustomer"))
table(churn_GBM_test_pred1$prediction)

# result4
churn_matrix4 <- confusionMatrix(factor(churn_GBM_test_pred1$prediction), 
                                 factor(churn_GBM_test_pred1$Attrition_Flag), 
                                 positive = "AttritedCustomer")
churn_matrix4

ggplot(as.data.frame(churn_matrix4$table)) +
  geom_raster(aes(x = Reference, y = Prediction, fill = Freq)) +
  geom_text(aes(x = Reference, y = Prediction, label = Freq)) +
  scale_fill_gradient2(low = "darkred", high = "pink",
                       na.value = "grey", name = "Freq") +
  scale_x_discrete(name = "Actual Class") +
  scale_y_discrete(name = "Predicted Class") +
  ggtitle("Gradient Boosting Tree Confusion Matrix") +
  theme(plot.title = element_text(hjust = .5, size = 10, face = "bold"))

# feature importance
# summary(churn_GBM)
var_imp4 <- varImp(churn_GBM1, n.trees = 500)$importance %>% 
  arrange(desc(Overall))
kable(head(var_imp4))

ggplot(var_imp4, aes(x = reorder(rownames(var_imp4), Overall), y = Overall)) +
  geom_point(color = "violet", size = 6, alpha = 1) +
  geom_segment(aes(x = rownames(var_imp4), xend = rownames(var_imp4), 
                   y = 0, yend = Overall), color = "skyblue") +
  xlab("Variable") +
  ylab("Overall Importance") +
  theme_light() +  
  coord_flip()
```

### vi. Gradient Boosting Tree Train NA‚ú®  
> Original data set with Training Set Missing Value Replaced and only replaced missing values in the training set  

The model has 0.9691 of accuracy and 0.8805 in kappa, and recall is 0.8545 

```{r GBM2}
# data split
index2 <- createDataPartition(BankChurners1$Attrition_Flag, 
                              p = .7, list = FALSE, times = 1)
 train2 <- BankChurners1[index,]
 test2 <- BankChurners1[-index,]
 table(is.na(train2))
 
# replace missing values in the training set
train2 <- VIM::kNN(train2, 
               variable = c("Dependent_count", "Education_Level", 
                           "Marital_Status", "Income_Level", 
                           "Months_Inactive_12_mon", "Contacts_Count_12_mon",
                           "Total_Revolving_Bal", "Total_Amt_Chng_Q4_Q1",
                           "Total_Ct_Chng_Q4_Q1", "Avg_Utilization_Ratio"),
              k = 10)
# summary(train2)
train2 <- train2[,-c(21:30)]
table(is.na(train2))

# train5
churn_GBM2 <- train(
  form = factor(Attrition_Flag) ~.,
  data = train2,
  trControl = trainControl(method = "cv",
                           number = 10,
                           classProbs = TRUE),
  method = "gbm",
  tuneLength = 10,
  verbose = FALSE
)
# churn_GBM2
kable(churn_GBM2$bestTune)
plot(churn_GBM2)

# test5
churn_GBM_pred2 <- predict(churn_GBM2, test, type = "prob")
churn_GBM_test_pred2 <- cbind(churn_GBM_pred2, test2)
churn_GBM_test_pred2 <- churn_GBM_test_pred2 %>% 
  mutate(prediction = if_else(AttritedCustomer > ExistingCustomer, 
                              "AttritedCustomer", "ExistingCustomer"))
table(churn_GBM_test_pred2$prediction)

# result5
churn_matrix5 <- confusionMatrix(factor(churn_GBM_test_pred2$prediction), 
                                 factor(churn_GBM_test_pred2$Attrition_Flag), 
                                 positive = "AttritedCustomer")
churn_matrix5

ggplot(as.data.frame(churn_matrix5$table)) +
  geom_raster(aes(x = Reference, y = Prediction, fill = Freq)) +
  geom_text(aes(x = Reference, y = Prediction, label = Freq)) +
  scale_fill_gradient2(low = "darkred", high = "lavender",
                       na.value = "grey", name = "Freq") +
  scale_x_discrete(name = "Actual Class") +
  scale_y_discrete(name = "Predicted Class") +
  ggtitle("Gradient Boosting Tree Confusion Matrix") +
  theme(plot.title = element_text(hjust = .5, size = 10, face = "bold"))
```

### vii. Gradient Boosting Tree Total NA & SMOTE
> Original data set with Total Missing Value Replaced and Resampling in the Training Set  

The model has 0.9635 of accuracy and 0.8668 in kappa, which is similar to the previous result. The recall is 0.9078 üéâ (Mcnemar's Test P-Value : 0.02545 üÖø) 

```{r GBM3}
# data split
index <- createDataPartition(BankChurners2$Attrition_Flag, 
                             p = .7, list = FALSE, times = 1)
train <- BankChurners2[index,]
test <- BankChurners2[-index,]

# train6
churn_GBM3 <- train(
  form = factor(Attrition_Flag) ~.,
  data = train,
  trControl = trainControl(method = "cv",
                           number = 10,
                           classProbs = TRUE,
                           sampling = "smote"),
  method = "gbm",
  tuneLength = 10,
  verbose = FALSE
)
# kable(churn_GBM3$bestTune)
# plot(churn_GBM3)

# test6
churn_GBM_pred3 <- predict(churn_GBM3, test, type = "prob")
churn_GBM_test_pred3 <- cbind(churn_GBM_pred3, test)
churn_GBM_test_pred3 <- churn_GBM_test_pred3 %>% 
  mutate(prediction = if_else(AttritedCustomer > ExistingCustomer, 
                              "AttritedCustomer", "ExistingCustomer"))
table(churn_GBM_test_pred3$prediction)

# result6
churn_matrix6 <- confusionMatrix(factor(churn_GBM_test_pred3$prediction), 
                                 factor(churn_GBM_test_pred3$Attrition_Flag), 
                                 positive = "AttritedCustomer")
churn_matrix6

ggplot(as.data.frame(churn_matrix6$table)) +
  geom_raster(aes(x = Reference, y = Prediction, fill = Freq)) +
  geom_text(aes(x = Reference, y = Prediction, label = Freq)) +
  scale_fill_gradient2(low = "darkred", high = "firebrick",
                       na.value = "grey", name = "Freq") +
  scale_x_discrete(name = "Actual Class") +
  scale_y_discrete(name = "Predicted Class") +
  ggtitle("Gradient Boosting Tree & SMOTE Confusion Matrix") +
  theme(plot.title = element_text(hjust = .5, size = 10, face = "bold"))

# feature importance
# summary(churn_GBM)
var_imp5 <- varImp(churn_GBM3, n.trees = 500)$importance %>% 
  arrange(desc(Overall))
kable(head(var_imp5))

ggplot(var_imp4, aes(x = reorder(rownames(var_imp5), Overall), y = Overall)) +
  geom_point(color = "powderblue", size = 6, alpha = 1) +
  geom_segment(aes(x = rownames(var_imp5), xend = rownames(var_imp5), 
                   y = 0, yend = Overall), color = "plum1") +
  xlab("Variable") +
  ylab("Overall Importance") +
  theme_light() +  
  coord_flip()
```

### viii. üåúNeural Network Total NAüïäÔ∏è
> Original data set with Total Missing Value Replaced  

The model has 0.9351 of accuracy and 0.7446 in kappa, and recall is 0.7277üòï. The ROC curve looks greatüèÑ
ROC result description: _____.

```{r NNET}
# train7
churn_NNET <- train(
  form = factor(Attrition_Flag) ~.,
  data = train,
  trControl = trainControl(method = "cv",
                           number = 10,
                           classProb =TRUE),
  method = "nnet",
  preProcess = c("center", "scale"),
  tuneLength = 5,
  trace= FALSE
)
plot(churn_NNET)

# test7
churn_NNET_pred <- predict(churn_NNET, test, type = "prob")
churn_NNET_test_pred <- cbind(churn_NNET_pred, test)
churn_NNET_test_pred <- churn_NNET_test_pred %>% 
  mutate(prediction = if_else(AttritedCustomer > ExistingCustomer, 
                              "AttritedCustomer", "ExistingCustomer"))
table(churn_NNET_test_pred$prediction)

roc_NNET <- pROC::roc(factor(churn_NNET_test_pred$Attrition_Flag), 
                      churn_NNET_test_pred$ExistingCustomer)
plot(roc_NNET)

# result7
churn_matrix7 <- confusionMatrix(factor(churn_NNET_test_pred$prediction), 
                                 factor(churn_NNET_test_pred$Attrition_Flag), 
                                 positive = "AttritedCustomer")
churn_matrix7

ggplot(as.data.frame(churn_matrix7$table)) +
  geom_raster(aes(x = Reference, y = Prediction, fill = Freq)) +
  geom_text(aes(x = Reference, y = Prediction, label = Freq)) +
  scale_fill_gradient2(low = "darkred", high = "plum1",
                       na.value = "grey", name = "Freq") +
  scale_x_discrete(name = "Actual Class") +
  scale_y_discrete(name = "Predicted Class") +
  ggtitle("Neural Network Confusion Matrix") +
  theme(plot.title = element_text(hjust = .5, size = 10, face = "bold"))
```

### ix. Neural Network Total NA & SMOTE
> Original data set with total missing value replaced & SMOTE resampling
  
The model has _____ of accuracy and _____ in kappa, which is ______ to the previous model. The recall is _____, _____ compared to the prior model. 
  
ROC result description: _____.

```{r NNET2}
# train8
churn_NNET2 <- train(
  form = factor(Attrition_Flag) ~.,
  data = train,
  trControl = trainControl(method = "cv",
                           number = 10,
                           classProb = TRUE,
                           sampling = "smote"),
  method = "nnet",
  preProcess = c("center", "scale"),
  tuneLength = 5,
  trace= FALSE
)
plot(churn_NNET2)

# test8
churn_NNET_pred2 <- predict(churn_NNET2, test, type = "prob")
churn_NNET_test_pred2 <- cbind(churn_NNET_pred2, test)
churn_NNET_test_pred2 <- churn_NNET_test_pred2 %>% 
  mutate(prediction = if_else(AttritedCustomer > ExistingCustomer, 
                              "AttritedCustomer", "ExistingCustomer"))
table(churn_NNET_test_pred2$prediction)

roc_NNET <- pROC::roc(factor(churn_NNET_test_pred2$Attrition_Flag), 
                      churn_NNET_test_pred2$ExistingCustomer)
plot(roc_NNET2)

# result8
churn_matrix8 <- confusionMatrix(factor(churn_NNET_test_pred2$prediction), 
                                 factor(churn_NNET_test_pred2$Attrition_Flag), 
                                 positive = "AttritedCustomer")
churn_matrix8

ggplot(as.data.frame(churn_matrix8$table)) +
  geom_raster(aes(x = Reference, y = Prediction, fill = Freq)) +
  geom_text(aes(x = Reference, y = Prediction, label = Freq)) +
  scale_fill_gradient2(low = "darkred", high = "maroon",
                       na.value = "grey", name = "Freq") +
  scale_x_discrete(name = "Actual Class") +
  scale_y_discrete(name = "Predicted Class") +
  ggtitle("Neural Network Confusion Matrix") +
  theme(plot.title = element_text(hjust = .5, size = 10, face = "bold"))
```

## VI. Result   
Overall, üå≥random forest model has the best performanceüèÜ compared to gradient boosting tree and neural networküå≤, especially after replacing total missing values and using SMOTE to fix üîß the imbalanced data set issue. (**iv.RF TOTAL NA & SMOTE**)

```{r comparison}
comparison <- matrix(c(0.9661, 0.8683, 0.8381, 0.9585, 0.8409, 0.8308, 0.9575, 0.8341, 
                       0.8062, 0.9549, 0.8423, 0.9344, 0.9681, 0.8781, 0.8648, 0.9691,  
                       0.8805, 0.8545, 0.9635, 0.8668, 0.9078, 0.9351, 0.7446, 0.7277,
                       0.0000, 0.0000, 0.0000),
                     ncol = 3, byrow = TRUE)
colnames(comparison) <- c("Accuracy", "Kappa", "Recall")
rownames(comparison) <- c("i.RF TOTAL NA", "ii.RF TOTAL NA & VAR", "iii.RF TRAIN NA", 
                          "iv.RF TOTAL NA & SMOTE", "v.GBT TOTAL NA", "vi.GBT TRAIN NA", 
                          "vii.GBT TOTAL NA & SMOTE", "viii.NNET TOTAL NA", "ix.NNET TOTAL NA & SMOTE")
comparison <- as.data.frame.matrix(comparison)
kable(comparison) %>% 
  row_spec(4, color = "white", background = "#bdaeea")
  
```
The original model from Kaggle has 0.62 for recall, so ü§†my models did improve the performance of predicting churned customersü•≥. They can help companies to identify potential customer churn with higher success rate. The neural network model _____. Based on the variable importance rates, customers' transaction numbers and amounts, changes in transaction amount, and total product held by customers are the most important‚≠ê predicting variables in those models. The demographic factors are not important in those models though.  

Limitations:



















